{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to scikit-learn: basic model hyper-parameters tuning\n",
    "\n",
    "In this lecture note, we aim at:\n",
    "* illustrate the influence of changing model parameters;\n",
    "* illustrate how to tune these hyper-parameters;\n",
    "* evaluate the model performance together with hyper-parameters tuning.\n",
    "\n",
    "## Recall of basic preprocessing and model fitting\n",
    "\n",
    "In the previous notebook, we show how to preprocessed different type of data\n",
    "and integrate this preprocessing in a machine learning pipeline containing a\n",
    "predictor.\n",
    "\n",
    "We will recall this example. First, we will load the data and organize it\n",
    "into a `data` and a `target` variable. The ultimate goal is to train a\n",
    "predictor able to estimate the wages from different censing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"https://www.openml.org/data/get_csv/1595261/adult-census.csv\")\n",
    "# Or use the local copy:\n",
    "# df = pd.read_csv('../datasets/adult-census.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "target_name = \"class\"\n",
    "target = df[target_name].to_numpy()\n",
    "data = df.drop(columns=target_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset is loaded, we will split it into a training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test, target_train, target_test = train_test_split(\n",
    "    data, target, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data split, we can define our preprocessing to transform differently\n",
    "the numerical and categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# note that we implicitely drop the column \"fnlwgt\"\n",
    "binary_encoding_columns = ['sex']\n",
    "one_hot_encoding_columns = ['workclass', 'education', 'marital-status',\n",
    "                            'occupation', 'relationship', 'race',\n",
    "                            'native-country']\n",
    "scaling_columns = ['age', 'capital-gain', 'capital-loss', 'hours-per-week',\n",
    "                   'education-num']\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('binary-encoder', OrdinalEncoder(), binary_encoding_columns),\n",
    "    ('one-hot-encoder', OneHotEncoder(handle_unknown='ignore'),\n",
    "     one_hot_encoding_columns),\n",
    "    ('standard-scaler', StandardScaler(), scaling_columns)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the preprocessing, we will use a linear classifier (i.e.\n",
    "logistic regression) to predict whether or not a person earn more than 50,000\n",
    "dollars a year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(preprocessor, LogisticRegression(max_iter=1000))\n",
    "model.fit(df_train, target_train)\n",
    "print(\n",
    "    f\"The R2 score using a {model.__class__.__name__} is \"\n",
    "    f\"{model.score(df_test, target_test):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The issue of having the best model parameters\n",
    "\n",
    "When using the `LoghisticRegression` classifier, one could notice that we are\n",
    "using the default parameters by omitting setting explicitly these parameters.\n",
    "\n",
    "For such classifier, the parameter `C` is governing the penalty; in other\n",
    "words, how much our model should \"trust\" (or fit) the training data.\n",
    "\n",
    "Therefore, the default value of `C` is never certified to give the best\n",
    "model.\n",
    "\n",
    "We can make a quick experiment by changing the value of `C` and see the\n",
    "impact of this parameter on the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 1\n",
    "model = make_pipeline(preprocessor, LogisticRegression(C=C, max_iter=1000))\n",
    "model.fit(df_train, target_train)\n",
    "print(\n",
    "    f\"The R2 score using a {model.__class__.__name__} is \"\n",
    "    f\"{model.score(df_test, target_test):.2f} with alpha={C}\"\n",
    ")\n",
    "\n",
    "C = 1e-5\n",
    "model = make_pipeline(preprocessor, LogisticRegression(C=C, max_iter=1000))\n",
    "model.fit(df_train, target_train)\n",
    "print(\n",
    "    f\"The R2 score using a {model.__class__.__name__} is \"\n",
    "    f\"{model.score(df_test, target_test):.2f} with alpha={C}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the best model hyper-parameters via exhaustive parameters search\n",
    "\n",
    "We see that the parameter `C` as a significative impact on the model\n",
    "performance and that finding the best value for this parameter is crucial.\n",
    "However, this parameter should be tuned with cross-validation such that\n",
    "we find a parameter. In short, we will set the parameter, train our model\n",
    "on some data, and evaluate the performance of our model on some left out\n",
    "data. Ideally, we will select the parameter leading to the optimal\n",
    "performance on the testing set. Scikit-learn provides a `GridSearchCV`\n",
    "estimator which will handle the cross-validation for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = make_pipeline(preprocessor, LogisticRegression(max_iter=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see that we need to provide the name of the parameter to be set.\n",
    "Thus, we can use the method `get_params()` to have the list of the parameters\n",
    "of the model which can set during the grid-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The hyper-parameters are for a logistic regression model are:\")\n",
    "for param_name in LogisticRegression().get_params().keys():\n",
    "    print(param_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The hyper-parameters are for the full-pipeline are:\")\n",
    "for param_name in model.get_params().keys():\n",
    "    print(param_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter `'logisticregression__C'` is the parameter for which we would\n",
    "like different values. Let see how to use the `GridSearchCV` estimator for\n",
    "doing such search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "param_grid = {'logisticregression__C': np.linspace(1e-5, 1, num=5)}\n",
    "model_grid_search = GridSearchCV(model, param_grid=param_grid)\n",
    "model_grid_search.fit(df_train, target_train)\n",
    "print(\n",
    "    f\"The R2 score using a {model_grid_search.__class__.__name__} is \"\n",
    "    f\"{model_grid_search.score(df_test, target_test):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `GridSearchCV` estimator takes a `param_grid` parameter which defines\n",
    "all possible parameters combination. Once the grid-search fitted, it can be\n",
    "used as any other predictor by calling `predict` and `predict_proba`.\n",
    "Internally, it will use the model with the best parameters found during\n",
    "`fit`. You can now about these parameters by looking at the attribute\n",
    "`best_params_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The best set of parameters is: {model_grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters during the grid-search need to be specificy. Instead, one\n",
    "could randomly generate (following a specific distribution) the parameter\n",
    "candidates. The `RandomSearchCV` allows for such stochastic search. It can\n",
    "be used similarly to the `GridSearchCV` but one has to specified the\n",
    "distributions instead of the parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distributions = {'logisticregression__C': uniform(loc=50, scale=100)}\n",
    "model_grid_search = RandomizedSearchCV(\n",
    "    model, param_distributions=param_distributions, n_iter=5\n",
    ")\n",
    "model_grid_search.fit(df_train, target_train)\n",
    "print(\n",
    "    f\"The R2 score using a {model_grid_search.__class__.__name__} is \"\n",
    "    f\"{model_grid_search.score(df_test, target_test):.2f}\"\n",
    ")\n",
    "print(f\"The best set of parameters is: {model_grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on search efficiency\n",
    "\n",
    "Be aware that sometimes, scikit-learn provides some `EstimatorCV` classes\n",
    "which will perform internally the cross-validation in such way that it will\n",
    "more computationally efficient. We can give the example of the\n",
    "`LogisticRegressionCV` which can be used to find the best `alpha` in a more\n",
    "efficient way than what we previously did with the `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# define the different alphas to try out\n",
    "param_grid = {\"C\": (0.1, 1.0, 10.0)}\n",
    "\n",
    "model = make_pipeline(preprocessor, LogisticRegressionCV(Cs=param_grid['C'],\n",
    "                                                         max_iter=1000))\n",
    "start = time.time()\n",
    "model.fit(df_train, target_train)\n",
    "print(f\"Time elapsed to train LogisticRegressionCV: \"\n",
    "      f\"{time.time() - start:.3f} seconds\")\n",
    "\n",
    "model = make_pipeline(\n",
    "    preprocessor, GridSearchCV(LogisticRegression(max_iter=1000),\n",
    "                               param_grid=param_grid)\n",
    ")\n",
    "start = time.time()\n",
    "model.fit(df_train, target_train)\n",
    "print(f\"Time elapsed to make a grid-search on LogisticRegression: \"\n",
    "      f\"{time.time() - start:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining evaluation and hyper-parameters search\n",
    "\n",
    "We saw that we are using a cross-validation for searching the best model\n",
    "parameters. In addition, we previously saw that we can use cross-validation\n",
    "to evaluate the model performance. If we would like to combine both aspects,\n",
    "one needs to perform \"nested\" cross-validation. The \"outer\" cross-validation\n",
    "will be applied to assess the model while the \"inner\" cross-validation will\n",
    "set the hyper-parameters of the model on the data set provided by the \"outer\"\n",
    "cross-validation. In practice, it is equivalent of including, `GridSearchCV`,\n",
    "`RandomSearchCV`, or any `EstimatorCV` in a `cross_val_score` or\n",
    "`cross_validate` function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model = make_pipeline(preprocessor, LogisticRegressionCV(max_iter=1000))\n",
    "score = cross_val_score(model, data, target)\n",
    "print(f\"The R2 score is: {score.mean():.2f} +- {score.std():.2f}\")\n",
    "print(f\"The different scores obtained are: \\n{score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be aware that such training might involve a variation of the hyper-parameters\n",
    "of the model. When analyzing such model, you should not only look at the\n",
    "overall model performance but look at the hyper-parameters variations as\n",
    "well."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "notebooks//ipynb,markdown_files//md,python_scripts//py:percent",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
