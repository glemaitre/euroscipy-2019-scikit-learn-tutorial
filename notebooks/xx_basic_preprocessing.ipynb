{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Introduction to scikit-learn: basic preprocessing for basic model fitting\n",
    "\n",
    "In this lecture note, we will aim at introducing:\n",
    "* the difference between numerical and categorical variables;\n",
    "* the importance of scaling numerical variables;\n",
    "* the way to encode categorical variables;\n",
    "* combine different preprocessing on different type of data;\n",
    "* evaluate the performance of a model via cross-validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduce the dataset\n",
    "\n",
    "To this aim, we will use data from the 1985 \"Current Population Survey\"\n",
    "(CPS). The goal with this data is to regress wages from heterogeneous data\n",
    "such as age, experience, education, family information, etc.\n",
    "\n",
    "Let's first load the data located in the `datasets` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(os.path.join('datasets', 'cps_85_wages.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly have a look at the head of the dataframe to check the type\n",
    "of available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target in our study will be the \"WAGE\" columns while we will use the\n",
    "other columns to fit a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "target_name = \"WAGE\"\n",
    "target = df[target_name].to_numpy()\n",
    "data = df.drop(columns=target_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the number of samples and the number of features available in\n",
    "the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"The dataset contains {data.shape[0]} samples and {data.shape[1]} \"\n",
    "    \"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work with numerical data\n",
    "\n",
    "The most intuitive type of data in machine learning which can (almost)\n",
    "directly be used in machine learning are known as numerical data. We can\n",
    "quickly have a look at such data by selecting the subset of columns from\n",
    "the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(data.columns)\n",
    "numerical_columns = ['AGE', 'EDUCATION', 'EXPERIENCE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this subset of data to fit linear regressor to infer the wage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_numeric = data[numerical_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building a machine learning model, it is important to leave out a\n",
    "subset of the data which we can use later to evaluate the trained model.\n",
    "The data used to fit a model a called training data while the one used to\n",
    "assess a model are called testing data.\n",
    "\n",
    "Scikit-learn provides an helper function `train_test_split` which will\n",
    "split the dataset into a training and a testing set. It will ensure that\n",
    "the data are shuffled before splitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    data_numeric, target, random_state=42\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"The training dataset contains {data_train.shape[0]} samples and \"\n",
    "    f\"{data_train.shape[1]} features\"\n",
    ")\n",
    "print(\n",
    "    f\"The testing dataset contains {data_test.shape[0]} samples and \"\n",
    "    f\"{data_test.shape[1]} features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We will build a Support Vector Machine (SVM) which is a linear model. The\n",
    "`fit` method is called to train the data and only the training data should\n",
    "be given for this purpose.\n",
    "To evaluate our model, we can use the method `score`. It will compute the\n",
    "coefficient of determination R2 when dealing with a regression problem.\n",
    "\n",
    "In addition, we checking the time required to train the model and internally\n",
    "check the number of iterations done by the solver to find a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "model = LinearSVR()\n",
    "start = time.time()\n",
    "model.fit(data_train, target_train)\n",
    "elapsed_time = time.time() - start\n",
    "print(\n",
    "    f\"The R2 score using a {model.__class__.__name__} is \"\n",
    "    f\"{model.score(data_test, target_test):.2f} with a fitting time of \"\n",
    "    f\"{elapsed_time:.3f} seconds in {model.n_iter_} iterations\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should not the `ConvergenceWarning` which inform us that our model stopped\n",
    "learning since it reaches the maximum number of iterations allowed by the\n",
    "user. This could potentially be detrimental for the model accuracy. We can\n",
    "follow the (bad) advice given in the warning message and increase the maximum\n",
    "number of iterations allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = LinearSVR(max_iter=50000)\n",
    "start = time.time()\n",
    "model.fit(data_train, target_train)\n",
    "elapsed_time = time.time() - start\n",
    "print(\n",
    "    f\"The R2 score using a {model.__class__.__name__} is \"\n",
    "    f\"{model.score(data_test, target_test):.2f} with a fitting time of \"\n",
    "    f\"{elapsed_time:.3f} seconds in {model.n_iter_} iterations\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe an increase in performance add the cost of a longer training.\n",
    "Instead of increasing the number of iterations, we could instead know a bit\n",
    "more about the SVR model and known that it is expecting input data to be\n",
    "scaled before to start training. A range of preprocessing algorithms in\n",
    "scikit-learn allows to transform the input data before to train a model.\n",
    "We can easily combine these sequential operation with a scikit-learn\n",
    "`Pipeline` which will chain the operations and can be used as any other\n",
    "classifier or regressor. The helper function `make_pipeline` will create\n",
    "a `Pipeline` by giving the successive transformations to perform.\n",
    "\n",
    "In our case, we will standardize the data and then train a linear SVR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(StandardScaler(), LinearSVR())\n",
    "start = time.time()\n",
    "model.fit(data_train, target_train)\n",
    "elapsed_time = time.time() - start\n",
    "print(\n",
    "    f\"The R2 score using a {model.__class__.__name__} is \"\n",
    "    f\"{model.score(data_test, target_test):.2f} with a fitting time of \"\n",
    "    f\"{elapsed_time:.3f} seconds in {model[-1].n_iter_} iterations\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the training time and the number of iterations is much\n",
    "shorter while the accuracy is equivalent.\n",
    "\n",
    "In the previous example, we split the original data into a training set and\n",
    "a testing set. This strategy has several issues: in the setting where the\n",
    "amount of data is limited, the subset of data used to train or test will be\n",
    "small; and the splitting was done in a random manner and we have no\n",
    "information regarding the confidence of the results obtained.\n",
    "\n",
    "Therefore, we can use what cross-validation. Cross-validation consists in\n",
    "repeating this random splitting into training and testing sets and aggregate\n",
    "the model performance. By repeating the experiment, one can get a the\n",
    "fluctuation of the model performance.\n",
    "\n",
    "The function `cross_val_score` allows for such experimental protocol by\n",
    "giving the model, the data and the target. Since there is several\n",
    "cross-validation strategies, `cross_val_score` takes a parameter `cv` which\n",
    "defines the splitting strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "score = cross_val_score(model, data_numeric, target, cv=5)\n",
    "print(f\"The R2 score (mean +/- 1 std. dev.) is: \"\n",
    "      f\"{score.mean():.2f} +/- {score.std():.2f}\")\n",
    "print(f\"The different scores obtained are: \\n{score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting `cv=5` created 5 training and testing sets on which we trained and\n",
    "tested a model. This strategy is called K-fold cross-validation where `K`\n",
    "corresponds to the number of split.\n",
    "\n",
    "## Work with categorical data\n",
    "\n",
    "In the previous section, we dealt with data for which numerical algorithms\n",
    "are mathematically designed to work natively. However, real datasets contain\n",
    "type of data which do not belong to this category and will require some\n",
    "preprocessing. Such preprocessing will transform these data to be numerical\n",
    "and thus natively handled by machine learning algorithms.\n",
    "\n",
    "Categorical data are broadly encountered in data science. Numerical data is a\n",
    "continuous quantity corresponding to a real numbers while categorical data\n",
    "are represented as discrete values. For instance, the variable `SEX` in our\n",
    "previous dataset is a categorical variable because it encodes the data with\n",
    "the two categories `male` and `female`.\n",
    "\n",
    "In the remainder of this section, we will present different strategies to\n",
    "encode categorical data into numerical data which can be used by a\n",
    "machine-learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "    'SOUTH', 'SEX', 'UNION', 'RACE', 'OCCUPATION', 'SECTOR', 'MARR'\n",
    "]\n",
    "data_categorical = data[categorical_columns]\n",
    "print(data_categorical.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode categories having an ordering\n",
    "\n",
    "The most intuitive strategy is to encode each category by a numerical value.\n",
    "The `OrdinalEncoder` will transform the data in such manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "print(data_categorical.head())\n",
    "print(f\"The datasets is composed of {data_categorical.shape[1]} features\")\n",
    "encoder = OrdinalEncoder()\n",
    "data_encoded = encoder.fit_transform(data_categorical)\n",
    "\n",
    "print(f\"The dataset encoded contains {data_encoded.shape[1]} features\")\n",
    "print(data_encoded[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all categories have been encoded for each feature\n",
    "independently. We can also notice that the number of feature before and after\n",
    "the encoding is the same.\n",
    "\n",
    "However, one has to be careful when using this encoding strategy. The\n",
    "encoding imposed an order regarding the categories: 0 is smaller than 1 which\n",
    "is smaller than 2, etc. If the original categories did not have such order\n",
    "then this encoding is not adequate and you should use one-hot encoding\n",
    "instead.\n",
    "\n",
    "### Encode categories which do not have an ordering\n",
    "\n",
    "As previously stated, `OrdinalEncoder` is encoding categorical data having\n",
    "an ordering. In this case, the `OneHotEncoder` should be used. For a given\n",
    "feature, it will create as many new columns as categories. For a sample,\n",
    "the column corresponding to the category will be set to `1` while the other\n",
    "columns will be set to `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "print(data_categorical.head())\n",
    "print(f\"The datasets is composed of {data_categorical.shape[1]} features\")\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "data_encoded = encoder.fit_transform(data_categorical)\n",
    "\n",
    "print(f\"The dataset encoded contains {data_encoded.shape[1]} features\")\n",
    "print(data_encoded[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can notice that the number of features after the encoding is larger than\n",
    "in the original data.\n",
    "\n",
    "Once the encoding is done, we could integrate it inside a machine learning\n",
    "pipeline as in the case with numerical data. In the following, we train a\n",
    "linear classifier on the encoded data and check the performance of this\n",
    "machine learning pipeline using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = make_pipeline(OneHotEncoder(handle_unknown='ignore'), LinearSVR())\n",
    "score = cross_val_score(model, data_categorical, target)\n",
    "print(f\"The R2 score is: {score.mean():.2f} +/- {score.std():.2f}\")\n",
    "print(f\"The different scores obtained are: \\n{score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining different preprocessing on different data type\n",
    "\n",
    "In the previous section, we saw that we need to treat data specifically\n",
    "depending of their nature (i.e. numerical or categorical). We were capable\n",
    "of making the preprocessing in two sequential steps but we did not present\n",
    "any tool which could allow us to first preprocess the data depending of\n",
    "their type and later on use these preprocessed data to train a single\n",
    "machine learning model.\n",
    "\n",
    "Scikit-learn provides a `ColumnTransformer` which will dispatch some specific\n",
    "columns to a specific transformer.\n",
    "\n",
    "We can first define the columns depending on their data type:\n",
    "* binary encoding: it will corresponds to features for which categories can\n",
    "  be encoded by the values 0 or 1.\n",
    "* one-hot encoding: it will corresponds to features for which categories\n",
    "  do not have a particular ordered and should be a column should be created\n",
    "  for each category.\n",
    "* scaling: it will corresponds to the numerical features which will be\n",
    "  standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "binary_encoding_columns = ['MARR', 'SEX', 'SOUTH', 'UNION']\n",
    "one_hot_encoding_columns = ['OCCUPATION', 'SECTOR', 'RACE']\n",
    "scaling_columns = ['AGE', 'EDUCATION', 'EXPERIENCE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create our `ColumnTransfomer` by specifying a list of triplet\n",
    "(preprocessor name, transformer, columns). Finally, we can merge this\n",
    "\"preprocessor\" in a machine learning pipeline by adding a machine learning\n",
    "model (e.g. a linear model) after the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('binary-encoder', OrdinalEncoder(), binary_encoding_columns),\n",
    "    ('one-hot-encoder', OneHotEncoder(handle_unknown='ignore'),\n",
    "     one_hot_encoding_columns),\n",
    "    ('standard-scaler', StandardScaler(), scaling_columns)\n",
    "])\n",
    "model = make_pipeline(preprocessor, RidgeCV())\n",
    "score = cross_val_score(model, data, target)\n",
    "print(f\"The R2 score is: {score.mean():.2f} +- {score.std():.2f}\")\n",
    "print(f\"The different scores obtained are: \\n{score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can notice that the model, even more complex than in the previous\n",
    "sections, follow the same API meaning that it `fit` is called to preprocess\n",
    "the data and `score` is used to predict and check the model performance."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "notebooks//ipynb,markdown_files//md,python_scripts//py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
